---
title: "Research Report: Analyzing Historical Knowledge Graph Repairs"
author: "Nicolas Ferranti and Jorao Gomes Jr."
date: "2024-03-20"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Effective data quality management is pivotal in maintaining the utility
and reliability of knowledge graphs. Knowledge graphs, like Wikidata,
heavily depend on property constraints to identify and rectify data
errors. Historical data repairs can be beneficial for understanding what
are the main data issues and what are common repair patterns. In this
research report, we present the project of the course Advanced Data
Analysis with R, where we analyze historical repairs performed in
Wikidata. We also asked ourselves what is the role of humans and bots
when it comes to repairs in Wikidata. Preliminary results show that 2/3
of the repairs for a given constraint type are done by humans.

## Introduction

In the world of structured data representation, Knowledge Graphs are
powerful tools for organizing and connecting information in a smart way.
One common type is RDF (Resource Description Framework) Knowledge
Graphs, which provide a standardized method for expressing and linking
different data points across various areas.

At the top of the list of open knowledge graphs is Wikidata, a project
started by the Wikimedia Foundation. It's a place where people worldwide
contribute and manage structured data. Wikidata covers a wide range of
topics, from historical figures to scientific concepts. It's used in
many ways, such as helping search engines, providing information for
Wikipedia's infoboxes, and supporting data-driven research.

Keeping data accurate and relevant is crucial for Wikidata. Since
knowledge is always changing and evolving, it's important to constantly
check and correct any mistakes. Wikidata uses property constraints to
describe how properties should be used and to flag any potential
inconsistencies in the data. These constraints help ensure that the data
in Wikidata is of good quality and makes sense.

Understanding how data has been corrected in the past can be very
important in the context of Wikidata. By looking at past corrections, we
can learn about common problems and how they've been fixed. This can
help us predict repairs and prevent future mistakes, making sure that
Wikidata stays reliable over time.

In this research project, we're going to study how data has been fixed
in Wikidata over time using the techniques we've learned in the Advanced
Data Analysis with R course. The research questions driving this study
are twofold: first, we aim to investigate the feasibility of predicting
correction operations for specific constraint types. Second, we seek to
clarify the contribution of humans and bots to the Wikidata ecosystem.
By addressing these questions, we aim to contribute to the ongoing
discourse on data quality management in knowledge representation and
shed light on the roles of human and machine interventions in knowledge
graph maintenance. More specifically, our questions are:

**RQ1**: Is it possible to predict the action necessary to fix a
constraint based on its type?

**RQ2**: What is the role of humans and bots when it comes to repairs in
Wikidata?

Our hypotheses is that human users have a more active role to fix
constraint violations than bots, which are mostly used to perform fixed
routines to update data. All the data created by us in this project is
available on
[GitHub](https://github.com/joraojr/wikidata-human-bot-constraints).

## Data collection

The data used in this research was first published by Pellisier et al.
[1] when investigating how correction rules could be mined for Wikidata.
In our case, we are interested in a statistical analysis over this data.
The dataset contains 67M past corrections collected until July, 2018 for
ten different Wikidata constraint types.

All the data from the paper can be [freely
downloaded](https://figshare.com/articles/dataset/Wikidata_Constraints_Violations_-_July_2017/7712720)

## Descriptive analysis

The provided data presents a breakdown of historical repairs performed
in Wikidata across various constraint types. Each constraint type
represents a specific rule or condition that data within Wikidata must
adhere to. This dataset contains 9 different types of constraints.
Consider a Wikidata statement in the form of item-property-value (e.g.
Vienna-CapitalOf-Austria), the analysed constraints can be described as:

-   One-of: Only the specified values are allowed for the property.

-   Single-Value: The property generally only has a single value for an
    item.

-   ConflictsWith: Items using this property should not have a certain
    other statement.

-   Distinct values: Values for this property should be unique across
    all of Wikidata, and no other entity should have the same value in a
    statement for this property.

-   Inverse: The property has an inverse property, and values for the
    property should have a statement with the inverse property pointing
    back to the original item.

-   ValueRequiresStatement: Values for this property should have a
    certain other statement.

-   ValueType: The referenced item should be a subclass or instance of
    the given type.

-   Type: Items with the specified property should have the given type.

-   ItemRequiresStatement: Items using this property should have a
    certain other statement.

The repairs can be of 2 types: deletion and additions:

-   Deletion repairs occur when a user needs to remove information from
    the Wikidata, i.e. the information for this kind of constraint was
    wrongly inserted by a user.

-   Addition repairs occur when a piece of information is missing and a
    user needs to add this new information to repair the constraint
    violation.

The table displays the number of deletions and additions made to correct
data errors for each constraint type:

| Constraint Type        | \# of deletion | \# of addition |
|------------------------|----------------|----------------|
| One-Of                 | 13709          | 10123          |
| Single-Value           | 389285         | 62640          |
| ConflictsWith          | 465985         | 82275          |
| Distinct values        | 7432512        | 440426         |
| Inverse                | 276966         | 2794271        |
| ValueRequiresStatement | 477681         | 13980980       |
| ValueType              | 834236         | 31099209       |
| Type                   | 998922         | 16843712       |
| ItemRequiresStatement  | 357938         | 15474644       |

## Investigation and testing

### Prediction model

To investigate **RQ1**, we used a multinomial logistic regression model.
In the context of this work, this model can be utilized to predict the
action type (deletion, addition, or revalidation) based on the
constraint type in a given dataset. In this context, the classes of
constraint types considered are One-Of, ConflictsWith, Inverse, and
Single-Value.

Multinomial logistic regression extends the concept of logistic
regression to handle scenarios with more than two discrete outcome
classes. In our case, the outcome classes are the action types
associated with each constraint type. The dependent variable, or the
outcome variable, represents the action type, while the independent
variables, or predictors, are the constraint types.

The model aims to estimate the probabilities of each action type
(deletion, addition, or revalidation) given the constraint type. It does
so by fitting a separate logistic regression model for each action type,
and comparing it against a reference category (typically chosen as the
base outcome). The coefficients estimated by the model provide insights
into the relationship between the constraint types and the likelihood of
each action type occurring.

To train and evaluate our model, we followed the steps below:

```{r, echo=FALSE}
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
if (!require("data.table")) install.packages("data.table"); library("data.table")
if (!require("nnet")) install.packages("nnet"); library("nnet")
if (!require("caret")) install.packages("caret"); library("caret")
```

```{r}
set.seed(123)  # Set seed for reproducibility
split_ratio <- 0.8  # 80% for training, 20% for testing

```

We first pre-processed the data. We created a class called REVALIDATION.
For this work, REVALIDATION is a constraint type that needs to process
both action types DELETION and ADDITION. To check a bit more of the
pre-processing, please take a look at the "T2- Preprocessing" Rmd file.

Here we load the data after the pre-processing:

```{r}
df <-  subset(unique(
  read.csv(
    "preprocessed_file_final.tsv",
    sep = "\t",
    header = TRUE,
    col.names = c(
      "subject",
      "property",
      "type",
      "total_actions",
      "action_category"
    )
  )
),
type != "REVALIDATION" & type != "")

```

First glance at the data we had:

```{r}
apply(df, 2, function(x) length(unique(x)))

```

```{r}
table(df$action_category)
table(df$type)

```

```{r}
table_action_category <- table(df$action_category)
table_type <- table(df$type)

# Plot bar chart for action_category
barplot(table_action_category, main = "Distribution of Actions", col = "lightblue", xlab = "Action", ylab = "Total",)

# Plot bar chart for type
barplot(table_type, main = "Distribution of Constraints Types (Log-Log Scale)", col = "lightgreen", xlab = "Constraint Type", ylab = "Total", log="y")
```

From the previous data is possible to see that we have a huge amount of
data. The classes are imbalanced, i.e., the amount of data spread among
the classes is not equal. This can be a problem for our model since
imbalanced that can confuse our data and prioritize one class above the
others.

Now we analyze the relation between the actions type and constrains
types:

```{r}

# Create a table of frequencies
table_data <- table(df$action_category,df$type)

# Bar plot
barplot(table_data, beside = TRUE, legend.text = TRUE, col = c("lightblue", "lightgreen","lightgray"),
        main = "Action vs. Constraint Type",
        xlab = "action_category", ylab = "Frequency")
```

Is possible to see that the inverse class contains the majority of the
addition steps. Besides that, the constraint oneOf is the one with a
lower number of instances. SingleValue and conclictsWith are quite
similar in terms of data distribution.

Let's now check if the is any significant difference among the classes
and the action types. To do that we ran a chi-squared test:

```{r}
chisq.test(df$action_category,df$type)


```

From the p-value is possible to confirm that there is a significant
difference among the classes. Therefore, highlights to us the we can
continue with our prediction model.

Now we divide our model into training and testing sets. We performed it
in a stratified way since we had already checked that our data was not
balanced. The stratified sampling helps to preserve the percentage of
data of each class during the splitting of training and testing sets.

```{r}
# Stratified sampling based on 'type' and 'action_category'
df <- df %>%
  group_by(type, action_category) %>%
  sample_n(size = 0.1 * n()) %>%
  ungroup()
```

Double-checking the data distribution:

```{r}
# Check the distribution of 'type' and 'action_category' in the subsample
table(df$action_category)
table(df$type)

```

```{r}

# num_rows <- nrow(df)
# train_indices <- sample(1:num_rows, round(split_ratio * num_rows))
# train_data <- df[train_indices, ]
# test_data <- df[-train_indices, ]

# Create a list of data frames
stratified_groups <- df %>%
  group_split(type, action_category)

# Sample per group 
sample_within_group <- function(group_df, split_ratio) {
  num_rows <- nrow(group_df)
  train_indices <- sample(1:num_rows, round(split_ratio * num_rows))
  train_data <- group_df[train_indices, ]
  test_data <- group_df[-train_indices, ]
  list(train_data = train_data, test_data = test_data)
}

# Apply the function to each group
stratified_samples <- lapply(stratified_groups, sample_within_group, split_ratio)

# Combine the sampled groups back into a single data frame for both training and testing
train_data <- do.call(rbind, lapply(stratified_samples, function(x) x$train_data))
test_data <- do.call(rbind, lapply(stratified_samples, function(x) x$test_data))

```

Double-checking our training and testing sets:

```{r}
print("Train")
table(train_data$action_category)
table(train_data$type)

print("Test")
table(test_data$action_category)
table(test_data$type)
```

```{r}
# Create tables of frequencies for action_category and type in both datasets
table_action_category <- rbind(table(train_data$action_category), table(test_data$action_category))
table_type <- rbind(table(train_data$type), table(test_data$type))

# Calculate percentages
percent_action_category <- prop.table(table_action_category, margin = 2) * 100
percent_type <- prop.table(table_type, margin = 2) * 100

# Plot stacked bar chart for action_category
barplot(percent_action_category, col = c("lightblue", "lightgreen"),
        legend.text = TRUE, args.legend = list(x = "topright", bty = "n"),
        main = "Train/Test - Action Percentagem",
        xlab = "Action Category", ylab = "Percentage")

# Plot stacked bar chart for type
barplot(percent_type, col = c("lightblue", "lightgreen"),
        legend.text = TRUE, args.legend = list(x = "topright", bty = "n"),
        main = "Train/Test - Type Percentagem",
        xlab = "Type", ylab = "Percentage")


```

From the chart above, we can see that we were able to split our data
carefully among the classes. The percentage of data is preserved and now
we are ready to build or model.

```{r}

# multinom_model <- multinom(action_category ~ subject + property + type + total_actions, data = train_data, nthreads = 4)
multinom_model <- multinom(action_category ~ type , data = train_data, nthreads = 4)


```

We first tried to run the model using multiple independent variables
(subject, property, and type). However, the model was too big to run on
a local machine. Therefore, we only selected the action type as the
dependable variable and the type of the constraint as the independent
variable.

Summary of our model:

```{r}
summary(multinom_model)
```

Let's now evaluate how the model residuals are:

```{r}
# Obtain deviance residuals
residuals <- residuals(multinom_model, type = "deviance")

# Create a residual plot
par(mfrow = c(2, 2))  # Set up a 2x2 layout for multiple plots

# Get unique values of the response variable (action_category)
response_levels <- unique(train_data$action_category)

# Create a residual plot for each category
for (i in response_levels) {
  subset_residuals <- residuals[train_data$action_category == i]
  plot(fitted(multinom_model)[train_data$action_category == i], subset_residuals,
       main = paste("Residuals vs Fitted for", i),
       xlab = "Fitted Values", ylab = "Deviance Residuals")
  abline(h = 0, col = "red", lty = 2)  # Add a horizontal line at y = 0
}

# Plot a histogram of deviance residuals
hist(residuals, main = "Histogram of Deviance Residuals", xlab = "Deviance Residuals")
abline(v = 0, col = "red", lty = 2)  # Add a vertical line at x = 0

```

```{r}
# Get the coefficients
coefficients <- coef(multinom_model)

# Exponentiate the coefficients to obtain odds ratios
odds_ratios <- exp(coefficients)

# Print or view the odds ratios
print(odds_ratios)
```

The final step was to verify if our prediction model was performing well
with our testing dataset. To do that, we plot our confusion matrix
below:

```{r}

# probabilities <- predict(multinom_model, newdata = test_data, type = "response")
# predictions <- ifelse(probabilities > 0.5, "ADD", "DELETE")  # Adjust threshold as needed


# Make predictions on the test set
predictions <- predict(multinom_model, newdata = test_data, type = "class")

# Evaluate the model
conf_matrix <- confusionMatrix(predictions, as.factor(test_data$action_category))
print(conf_matrix)

```

```{r}

conf_matrix_df= as.data.frame(conf_matrix$table)
ggplot(conf_matrix_df, aes(Reference, Prediction, fill = Freq)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = Freq), vjust = 1, color = "black", size = 4) +
  scale_fill_gradient(low = "#EFEFEF", high = "#009194") +
  labs(x = "Predicted Class", y = "True Class",
       title = "Confusion Matrix",
       subtitle = "Multinomial Logistic Regression") +
  theme_minimal(base_size = 16) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#F4F4F4"),
        legend.position = "right")



```

```{r}
# Create a data frame from the confusion matrix table
conf_matrix_df <- as.data.frame(conf_matrix$table)

# Convert the frequencies to percentages within each row
conf_matrix_df_percentage <- conf_matrix_df %>%
  group_by(Prediction) %>%
  mutate(Percentage = Freq / sum(Freq) * 100) %>%
  ungroup()

# Plot the confusion matrix heatmap with row-wise percentage annotations
ggplot(conf_matrix_df_percentage, aes(Reference, Prediction, fill = Percentage)) +
  geom_tile(color = "white", size = 0.5) +
  geom_text(aes(label = paste0(round(Percentage, 1), "%")), vjust = 1, color = "black", size = 4) +
  scale_fill_gradient(low = "#EFEFEF", high = "#009194") +
  labs(x = "Predicted Class", y = "True Class",
       title = "Confusion Matrix",
       subtitle = "Multinomial Logistic Regression") +
  theme_minimal(base_size = 16) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#F4F4F4"),
        legend.position = "right")
```

The model is very good in predicting addition as addition. However, it
gets confused when it's necessary to classify deletion as deletion and
revalidation and revalidation.

The final f1 score for the whole model is the following:

```{r}
f1_score <- conf_matrix$byClass[, "F1"]
f1_score
```

After this data exploration, we conclude that our 1st model can predict
the steps necessary to fix these constraints. The f1-score and the
confusion matrix confirm these results.

### Humans x Bots

To investigate **RQ2**, we took the Single-value constraint as a target
and investigated its repairs. Unfortunately, the repairs dataset does
not contain any information regarding the user behind the repair,
however, the revision ID stored in the second column of the repair files
can be used to retrieve the respective user. Every revision ID has one
correspondent user in Wikidata. Therefore, we developed a users
[extractor](https://github.com/joraojr/wikidata-human-bot-constraints/blob/main/users_extractor.R)
R script to first create a proper URL from the revision ID and then send
a HTTP request to Wikidata retrieving a HTML page that is further parsed
into an object in order to retrieve the user ID. The main challenge here
is to manage number of requests to not overwhelm the server's capacity,
leading to a denial of service. After retrieving almost 370k users using
the script available in "users_extractor.R", we analized the file
"users_constraint-corrections_single.tsv".

We first read the file containing the extracted users:

```{r}
# Set the path to the users TSV file
users_file_path <- "users_constraint-corrections_single.tsv"

# Read the TSV file
data <- read.table(users_file_path, sep = "\t", header = FALSE)
col.names = c("Column1", "Column2")
colnames(data) <- col.names
```

Next, we initialize the counters and check the number of total users:

```{r}
dim(data) # Initialize counters
total_rows <- nrow(data)
bot_substring_count <- 0
non_bot_substring_count <- 0
```

Then the file is iterated to count how many user names have "bot" as a
substring, since all bots in Wikidata have this pattern.

```{r}
for (row in 1:total_rows) {
  # Check if "bot" is a substring in the second column using grep
  if (grepl("bot", data$Column2[row], ignore.case = TRUE)) {
    bot_substring_count <- bot_substring_count + 1
  } else {
    non_bot_substring_count <- non_bot_substring_count + 1
  }
}
```

We calculate the percentage of humans and bots related to the total.

```{r}
# Calculate percentages
bot_percentage <- (bot_substring_count / total_rows) * 100
non_bot_percentage <- (non_bot_substring_count / total_rows) * 100

# Print the results
cat("Number of rows:", total_rows, "\n")
cat("Number of strings in the second column with 'bot' as a substring:", bot_substring_count, "(", round(bot_percentage, 2), "%)\n")
cat("Number of strings in the second column without 'bot' as a substring:", non_bot_substring_count, "(", round(non_bot_percentage, 2), "%)\n")

```

Finally, we can plot a pie chart with the share of users and bots that
performed repairs for the Single-value constraint:

```{r}
if (!requireNamespace("scales", quietly = TRUE)) {
  install.packages("scales")
}
library(scales)
# Generate a pie chart
pie_counts <- c(bot_substring_count, non_bot_substring_count)
pie_labels <- c(paste("Bot Substring Count (", round(bot_percentage, 2), "%)"),
                paste("Non-Bot Substring Count (", round(non_bot_percentage, 2), "%)"))
# Create a pie chart with percentages
pie(pie_counts, labels = pie_labels, main = "Share of Bots x Humans - Single-value constraint", col = c("red", "green"))
```

Our preliminary results show that around 66% of the repairs for this
given constraint type are done by human users suggesting that the
hypothesis for **RQ2** is valid for the single-value constraint.

```{r}
value_counts <- table(data$Column2)

# Rank the top 10 most frequent values
top_10_values <- head(sort(value_counts, decreasing = TRUE), 10)

# Print the total occurrences and top 10 most frequent values
cat("Total occurrences of values in the second column:", sum(value_counts), "\n")
cat("Top 10 most frequent values in the second column:\n")
print(top_10_values)
```

The top 10 users who did most repairs shows that 4 out of the 10 most
contributors are bots. In a future step, a detailed analysis on repair
patterns of each user can determine if the humans are performing repairs
across different knowledge domains while bots are executing routine
repairs. What is certain for now is that the most contributor is a bot
named KrBot and it has twice the amount of repairs compared to the
second most contributor which is a human.

## Conclusion

In this research project, we investigated repair patters of historical
edits in Wikidata. We have partially extended a published dataset
including information on the users behind the repairs. Our first
analysis indicates that the model is good in predicting addition
repairs, and that 66% of the repairs performed for a subset of the
single-value constraint repairs are done by humans.

Some of the main challenges we faced are related to the server capacity
in handling multiple http requests to get the users and the definition
of the features we used to train the prediction model. Our conclusion
for **RQ2** is limited to the context of the single-value constraint and
requires more information if one would like to make any general
assertion about the overall behavior for the whole dataset. Regarding
the features used to train the model, more information about the actual
entities could be beneficial to generate more features to improve the
accuracy, such as incorporating class hierarchy and adding more
contextual information about the domain knowledge of each repair.

## References

[1] PELLISSIER TANON, Thomas; BOURGAUX, Camille; SUCHANEK, Fabian.
Learning how to correct a knowledge base from the edit history. In: The
World Wide Web Conference. 2019. p. 1465-1475.
