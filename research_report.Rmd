---
title: "Research Report: Analyzing Historical Knowledge Graph Repairs"
author: "Nicolas Ferranti and Jorao Gomes Jr."
date: "2024-03-20"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Abstract

Effective data quality management is pivotal in maintaining the utility
and reliability of knowledge graphs. Knowledge graphs, like Wikidata,
heavily depend on property constraints to identify and rectify data
errors. Historical data repairs can be beneficial for understanding what
are the main data issues and what are common repair patterns. In this
research report, we present the project of the course Advanced Data
Analysis with R, where we analyze historical repairs performed in
Wikidata. We also asked ourselves what is the role of humans and bots
when it comes to repairs in Wikidata. Preliminary results show that 2/3
of the repairs for a given constraint type are done by humans.

## Introduction

In the world of structured data representation, Knowledge Graphs are
powerful tools for organizing and connecting information in a smart way.
One common type is RDF (Resource Description Framework) Knowledge
Graphs, which provide a standardized method for expressing and linking
different data points across various areas.

At the top of the list of open knowledge graphs is Wikidata, a project
started by the Wikimedia Foundation. It's a place where people worldwide
contribute and manage structured data. Wikidata covers a wide range of
topics, from historical figures to scientific concepts. It's used in
many ways, such as helping search engines, providing information for
Wikipedia's infoboxes, and supporting data-driven research.

Keeping data accurate and relevant is crucial for Wikidata. Since
knowledge is always changing and evolving, it's important to constantly
check and correct any mistakes. Wikidata uses property constraints to
describe how properties should be used and to flag any potential
inconsistencies in the data. These constraints help ensure that the data
in Wikidata is of good quality and makes sense.

Understanding how data has been corrected in the past can be very
important in the context of Wikidata. By looking at past corrections, we
can learn about common problems and how they've been fixed. This can
help us predict repairs and prevent future mistakes, making sure that
Wikidata stays reliable over time.

In this research project, we're going to study how data has been fixed
in Wikidata over time using the techniques we've learned in the Advanced
Data Analysis with R course. The research questions driving this study
are twofold: first, we aim to investigate the feasibility of predicting
correction operations for specific constraint types. Second, we seek to
clarify the contribution of humans and bots to the Wikidata ecosystem.
By addressing these questions, we aim to contribute to the ongoing
discourse on data quality management in knowledge representation and
shed light on the roles of human and machine interventions in knowledge
graph maintenance. More specifically, our questions are:

**RQ1**: Is it possible to predict the action necessary to fix a
constraint based on its type?

**RQ2**: What is the role of humans and bots when it comes to repairs in
Wikidata?

Our hypotheses is that human users have a more active role to fix
constraint violations than bots, which can be used

## Data collection

The data used in this research was first published by Pellisier et al.
[1] when investigating how correction rules could be mined for Wikidata.
In our case, we are interested in a statistical analysis over this data.
The dataset contains 67M past corrections collected until July, 2018 for
ten different Wikidata constraint types.

All the data can be [freely
downloaded](https://figshare.com/articles/dataset/Wikidata_Constraints_Violations_-_July_2017/7712720)

## Descriptive analysis

The provided data presents a breakdown of historical repairs performed
in Wikidata across various constraint types. Each constraint type
represents a specific rule or condition that data within Wikidata must
adhere to. This dataset contains 9 different types of constraints. Consider a Wikidata statement in the form of item-property-value (e.g. Vienna-CapitalOf-Austria), the analysed constraints can be described as:

-   One-of: Only the specified values are allowed for the property.

-   Single-Value: The property generally only has a single value for an item.

-   ConflictsWith: Items using this property should not have a certain other statement.

-   Distinct values: Values for this property should be unique across all of Wikidata, and no other entity should have the same value in a statement
for this property.

-   Inverse: The property has an inverse property, and values for the property should have a statement with the inverse property pointing back to the
original item.

-   ValueRequiresStatement: Values for this property should have a certain other statement.

-   ValueType: The referenced item should be a subclass or instance of the given type.

-   Type: Items with the specified property should have the given type.

-   ItemRequiresStatement: Items using this property should have a certain other statement.

The repairs can be of 2 types: deletion and additions:

-   Deletion repairs occur when a user needs to remove information from
    the Wikidata, i.e. the information for this kind of constraint was
    wrongly inserted by a user.

-   Addition repairs occur when a piece of information is missing and a
    user needs to add this new information to repair the constraint.

The table displays the number of deletions and additions made to correct
data errors for each constraint type:

| Constraint Type        | \# of deletion | \# of addition |
|------------------------|----------------|----------------|
| One-Of                 | 13709          | 10123          |
| Single-Value           | 389285         | 62640          |
| ConflictsWith          | 465985         | 82275          |
| Distinct values        | 7432512        | 440426         |
| Inverse                | 276966         | 2794271        |
| ValueRequiresStatement | 477681         | 13980980       |
| ValueType              | 834236         | 31099209       |
| Type                   | 998922         | 16843712       |
| ItemRequiresStatement  | 357938         | 15474644       |

## Investigation and testing

### Humans x Bots

To investigate **RQ2**, we took the Single-value constraint as a target
and investigated its repairs. Unfortunately, the repairs dataset does
not contain any information regarding the user behind the repair,
however, the revision ID stored in the second column of the repair files
can be used to retrieve the respective user. Every revision ID has one
correspondent user in Wikidata. Therefore, we developed a users
extractor R script to first create a proper URL from the revision ID and
then send a HTTP request to Wikidata retrieving a HTML page that is
further parsed into an object in order to retrieve the user ID. The main
challenge here is to manage number of requests to not overwhelm the
server's capacity, leading to a denial of service. After retrieving
almost 370k users using the script available in "users_extractor.R", we
analized the file "users_constraint-corrections_single.tsv".

We first read the file containing the extracted users:

```{r}
# Set the path to the users TSV file
users_file_path <- "users_constraint-corrections_single.tsv"

# Read the TSV file
data <- read.table(users_file_path, sep = "\t", header = FALSE)
col.names = c("Column1", "Column2")
colnames(data) <- col.names
```

Next, we initialize the counters and check the number of total users:

```{r}
dim(data) # Initialize counters
total_rows <- nrow(data)
bot_substring_count <- 0
non_bot_substring_count <- 0
```

Then the file is iterated to count how many user names have "bot" as a
substring, since all bots in Wikidata have this pattern.

```{r}
for (row in 1:total_rows) {
  # Check if "bot" is a substring in the second column using grep
  if (grepl("bot", data$Column2[row], ignore.case = TRUE)) {
    bot_substring_count <- bot_substring_count + 1
  } else {
    non_bot_substring_count <- non_bot_substring_count + 1
  }
}
```

We calculate the percentage of humans and bots related to the total.

```{r}
# Calculate percentages
bot_percentage <- (bot_substring_count / total_rows) * 100
non_bot_percentage <- (non_bot_substring_count / total_rows) * 100

# Print the results
cat("Number of rows:", total_rows, "\n")
cat("Number of strings in the second column with 'bot' as a substring:", bot_substring_count, "(", round(bot_percentage, 2), "%)\n")
cat("Number of strings in the second column without 'bot' as a substring:", non_bot_substring_count, "(", round(non_bot_percentage, 2), "%)\n")

```

Finally, we can plot a pie chart with the share of users and bots that
performed repairs for the Single-value constraint:

```{r}
if (!requireNamespace("scales", quietly = TRUE)) {
  install.packages("scales")
}
library(scales)
# Generate a pie chart
pie_counts <- c(bot_substring_count, non_bot_substring_count)
pie_labels <- c(paste("Bot Substring Count (", round(bot_percentage, 2), "%)"),
                paste("Non-Bot Substring Count (", round(non_bot_percentage, 2), "%)"))
# Create a pie chart with percentages
pie(pie_counts, labels = pie_labels, main = "Share of Bots x Humans - Single-value constraint", col = c("red", "green"))
```

Our preliminary results show that around 66% of the repairs for this
given constraint type are done by human users...

## Conclusion

## References

[1] PELLISSIER TANON, Thomas; BOURGAUX, Camille; SUCHANEK, Fabian.
Learning how to correct a knowledge base from the edit history. In: The
World Wide Web Conference. 2019. p. 1465-1475.
